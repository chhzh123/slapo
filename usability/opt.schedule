shard_embedding()
shard_attention()
shard_mlp()
subsch["attention.output.dense"].decompose()
trace_attention()
subgraphs = sch.find(scaled_dot_product)
sch.replace(EfficientAttention(), subgraphs)
subgraph = sch.find(lambda x, bias: F.gelu(bias + x))
sch.fuse(subgraph, compiler="TorchInductor", name="BiasGeLU")
sch[path.replace("N", str(idx))].checkpoint(order_args_fn=order_args_fn)
sch[word_embed_name].sync(mode="fwd_pre", sync_op_or_fn=fwd_pre_hook)
sch[word_embed_name].sync(mode="fwd_post", sync_op_or_fn=fwd_post_hook)
sch[q].shard(["weight", "bias"], axis=0)
sch[k].shard(["weight", "bias"], axis=0)
sch[v].shard(["weight", "bias"], axis=0)
sch.sync(mode="bwd_post", sync_op_or_fn="all_reduce")
sch[out].shard("weight", axis=1)
sch[out].sync("fwd_post", sync_op_or_fn="all_reduce")
sch[l1].shard(["weight", "bias"], axis=0)
sch[l2].shard("weight", axis=1)
sch[l2].sync("fwd_post", sync_op_or_fn="all_reduce")
sch[l1].sync("bwd_post", sync_op_or_fn="all_reduce")
sch[name].decompose()
sch[dense].decompose()
sch.trace(tracer="huggingface", flatten=True)
subgraphs = sch.find(scaled_dot_product)
sch.replace(EfficientAttention(), subgraphs)
subgraph = sch.find(lambda x, bias: F.gelu(bias + x))
sch.fuse(subgraph, compiler="TorchInductor", name="BiasGeLU")
subgraph = sch.find(lambda x, bias, residual: call_module(ln, F.dropout(bias + x) + residual))
sch.fuse(subgraph, compiler="TorchInductor", name="LNResidual")